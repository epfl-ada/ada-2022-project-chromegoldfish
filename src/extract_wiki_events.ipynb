{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of data from [timeline of events](https://en.wikipedia.org/wiki/Timeline_of_the_21st_century)\n",
    "\n",
    "Need to download the xml file on this [link](https://en.wikipedia.org/wiki/Special:Export/Timeline_of_the_21st_century)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arbenmiftari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arbenmiftari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/wiki_events.xml'\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "Bs_data = BeautifulSoup(data, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_txt = Bs_data.text\n",
    "idx = plain_txt.find('===[[2005]]===')\n",
    "text = plain_txt[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('\\n\\n', '\\n')\n",
    "text = text.replace('[', '')\n",
    "text = text.replace(']', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_txt = split_txt.copy()\n",
    "new_list=[]\n",
    "for i,txt in enumerate(copy_txt):\n",
    "    if 'see' in txt:\n",
    "        continue\n",
    "    if 'See' in txt:\n",
    "        break\n",
    "    if  '2020s' in txt:\n",
    "        continue\n",
    "    if  '2010s' in txt:\n",
    "        continue\n",
    "    if '--' in txt:\n",
    "        continue\n",
    "    if '===' not in txt:\n",
    "        new_list.append(txt[2:])\n",
    "    else:\n",
    "        new_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuples = []\n",
    "for i,element in enumerate(new_list):\n",
    "    month = ''\n",
    "    if '===' in element:\n",
    "        year = element[3:7]\n",
    "        continue\n",
    "    if any(month in element for month in list_months):\n",
    "        idx = element.find(':')\n",
    "        month = element[:idx]\n",
    "        element=element[idx+1:]\n",
    "    list_tuples.append((year,month, element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.DataFrame(list_tuples, columns =['Year','Day', 'Event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.to_csv('../datasets/wiki_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration to determine controversial videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Videos informations and state (num of likes/views) \n",
    "data_metadata_path = \"../data/yt_metadata_helper.feather\"\n",
    "df_metadata = pd.read_feather(data_metadata_path)\n",
    "df_metadata['dummy']=1\n",
    "df_metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(df):\n",
    "    check = False\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().values.any():\n",
    "            print(col)\n",
    "            check = True\n",
    "    if not check:\n",
    "            print('None')\n",
    "check_nan(df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.fillna(0, inplace=True)\n",
    "df_metadata.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_count = df_metadata.copy()\n",
    "df_cat_count = df_cat_count.groupby(\"categories\", as_index=False).sum()\n",
    "df_cat_count.drop(labels=[0,8,15], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_count.plot.bar(x='categories', y=['like_count', 'dislike_count'], logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_count['ratio']=df_cat_count['dislike_count']/(df_cat_count['like_count']+df_cat_count['dislike_count'])\n",
    "df_cat_count[['categories', 'ratio']].sort_values(by=['ratio'])\n",
    "df_cat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_count.plot.bar(x='categories', y='ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_metadata.copy()\n",
    "df['ratio']=df['dislike_count']/(df['like_count']+df['dislike_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio'].fillna(0, inplace=True)\n",
    "df['ratio'].isnull().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,3, figsize=(16,8), sharey=True)\n",
    "\n",
    "for cat,ax in zip(df['categories'].unique(), axs.ravel()):\n",
    "    df[df.categories==cat].hist(column='ratio', bins=20 ,ax=ax, log=True)\n",
    "    ax.set_title(cat)\n",
    "    ax.set_xlabel('ratio')\n",
    "    ax.set_ylabel('count')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the content for controversial videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/metadata_chunks/'\n",
    "csv_files = glob.glob(path+'*.csv.gz')\n",
    "df0 = pd.read_csv(csv_files[50], usecols=['categories', 'description', 'tags', 'title', 'dislike_count', 'like_count'])\n",
    "df0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing: removing unuseful columns, keep videos with at least 50 likes and dislikes\n",
    "#Keep the columns in which we are interested in\n",
    "df_raw = df0.drop_duplicates(subset=['title'])\n",
    "df_raw[\"video_info\"] = df_raw['title'].astype(str) +\": \"+ df_raw[\"description\"]\n",
    "df_raw = df_raw.drop(['title'],  axis=1)\n",
    "df_raw = df_raw.drop(['description'], axis=1)\n",
    "df_raw = df_raw[df_raw.categories=='News & Politics']\n",
    "df_raw = df_raw[df_raw.dislike_count+df_raw.like_count > 25]\n",
    "df_raw['ratio']=df_raw['dislike_count']/(df_raw['like_count']+df_raw['dislike_count'])\n",
    "df_raw['ratio'].fillna(0, inplace=True)\n",
    "df_raw = df_raw.drop(['dislike_count', 'like_count', 'categories'],  axis=1)\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only videos whose ratio is higher than than a certain quantile\n",
    "quantile = df_raw['ratio'].quantile(0.33)\n",
    "print(quantile)\n",
    "df_raw = df_raw[df_raw.ratio > quantile]\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing our dataframe to be clustered\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# we need to tokenize our texts.\n",
    "df[\"text\"] = df_raw['video_info']\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values and keep relevant columns\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    "\n",
    "print(f\"Original dataframe: {df_raw.shape}\")\n",
    "print(f\"Pre-processed dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our sentences into numerical vectors\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clusters and print Silhouette metrics using MBKmeans\n",
    "def mbkmeans_clusters(X, k, mb, print_silhouette_values):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=200,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(200):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
